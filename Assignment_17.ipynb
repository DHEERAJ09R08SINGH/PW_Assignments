{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "686d1557-d05b-45e9-a522-79d14c4dabc3",
   "metadata": {},
   "source": [
    "# Assignment_17 Questions & Answers :-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c59b81a-c6ce-4e5d-8193-70e4e057f4b0",
   "metadata": {},
   "source": [
    "### Q1. What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data.\n",
    "### Ans:-\n",
    "#### ### What is Web Scraping?\n",
    "\n",
    "**Web Scraping** is the process of extracting data from websites. This process involves retrieving the HTML content of web pages and parsing the information contained within the HTML tags to obtain structured data, which can be stored and analyzed further.\n",
    "\n",
    "### Why is Web Scraping Used?\n",
    "\n",
    "Web scraping is used because it allows for automated data collection from the web. Here are a few reasons why it is commonly used:\n",
    "\n",
    "1. **Data Collection**: To gather large amounts of data quickly and efficiently from various websites.\n",
    "2. **Market Research**: To monitor competitor prices, product details, and reviews.\n",
    "3. **Content Aggregation**: To collect and aggregate content from multiple sources, such as news articles, blogs, and forums.\n",
    "4. **Automation**: To automate repetitive tasks, such as checking stock prices or weather updates.\n",
    "\n",
    "### Areas Where Web Scraping is Used to Get Data\n",
    "\n",
    "1. **E-commerce and Retail**:\n",
    "   - **Price Monitoring**: Companies scrape prices from competitor websites to adjust their own pricing strategies.\n",
    "   - **Product Information**: Aggregators collect product details, reviews, and availability from various e-commerce sites to provide comparison services.\n",
    "\n",
    "2. **Real Estate**:\n",
    "   - **Property Listings**: Real estate websites scrape property data, such as prices, locations, and descriptions, from other sites to display comprehensive listings.\n",
    "   - **Market Trends**: Analysts scrape historical pricing data and current listings to analyze market trends and make investment decisions.\n",
    "\n",
    "3. **Social Media and News**:\n",
    "   - **Sentiment Analysis**: Companies scrape social media posts, comments, and reviews to analyze public sentiment about brands, products, or events.\n",
    "   - **Content Curation**: News aggregators scrape articles from multiple news sources to provide curated content in one place.\n",
    "\n",
    "### Example Code for Web Scraping\n",
    "\n",
    "Here’s a simple example using Python and the BeautifulSoup library to scrape headlines from a news website:\n",
    "\n",
    "```python\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the website to scrape\n",
    "url = 'https://example-news-website.com'\n",
    "\n",
    "# Send a GET request to the website\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the webpage\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find all headline elements (assuming they are in <h2> tags)\n",
    "headlines = soup.find_all('h2', class_='headline')\n",
    "\n",
    "# Extract and print the text of each headline\n",
    "for headline in headlines:\n",
    "    print(headline.text)\n",
    "```\n",
    "\n",
    "### Summary\n",
    "\n",
    "- **Web Scraping**: The process of extracting data from websites automatically.\n",
    "- **Uses**: Data collection, market research, content aggregation, automation, and more.\n",
    "- **Common Areas**: E-commerce for price monitoring, real estate for property listings, social media for sentiment analysis, and news for content curation.\n",
    "\n",
    "Web scraping is a powerful tool for gathering and analyzing data from the web, enabling businesses and researchers to gain insights and make informed decisions based on up-to-date information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc07a09-86b7-454c-a2be-7b341da0fb07",
   "metadata": {},
   "source": [
    "### Q2. What are the different methods used for Web Scraping?\n",
    "### Ans:-\n",
    "#### Web scraping can be accomplished using various methods, depending on the complexity of the website and the specific requirements of the data to be extracted. Here are some common methods used for web scraping:\n",
    "\n",
    "### 1. **Manual Scraping**\n",
    "\n",
    "- **Description**: Manually copying and pasting data from a website.\n",
    "- **Use Case**: Suitable for small-scale scraping or when automation is not necessary.\n",
    "- **Pros**: Simple and does not require any technical skills.\n",
    "- **Cons**: Time-consuming and not scalable for large datasets.\n",
    "\n",
    "### 2. **Using Regular Expressions**\n",
    "\n",
    "- **Description**: Utilizing regular expressions (regex) to match patterns in the HTML content and extract data.\n",
    "- **Use Case**: Effective for simple and predictable HTML structures.\n",
    "- **Pros**: Fast and can be integrated into various programming languages.\n",
    "- **Cons**: Difficult to maintain and not suitable for complex or dynamic web pages.\n",
    "\n",
    "### 3. **HTML Parsing Libraries**\n",
    "\n",
    "- **Description**: Using libraries like BeautifulSoup (Python) or Cheerio (JavaScript) to parse HTML and extract data.\n",
    "- **Use Case**: Commonly used for moderately complex scraping tasks where the HTML structure is relatively stable.\n",
    "- **Pros**: Easy to use, powerful, and handles HTML well.\n",
    "- **Cons**: Can be slow for very large web pages.\n",
    "\n",
    "**Example Using BeautifulSoup**:\n",
    "```python\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "url = 'https://example.com'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "data = soup.find_all('div', class_='example-class')\n",
    "\n",
    "for item in data:\n",
    "    print(item.text)\n",
    "```\n",
    "\n",
    "### 4. **Browser Automation Tools**\n",
    "\n",
    "- **Description**: Tools like Selenium or Puppeteer that control a web browser to navigate and interact with web pages.\n",
    "- **Use Case**: Ideal for scraping dynamic web pages that rely on JavaScript to load content.\n",
    "- **Pros**: Can handle JavaScript and complex interactions like clicking buttons or logging in.\n",
    "- **Cons**: Slower and more resource-intensive compared to other methods.\n",
    "\n",
    "**Example Using Selenium**:\n",
    "```python\n",
    "from selenium import webdriver\n",
    "\n",
    "driver = webdriver.Chrome(executable_path='/path/to/chromedriver')\n",
    "driver.get('https://example.com')\n",
    "\n",
    "data = driver.find_elements_by_class_name('example-class')\n",
    "\n",
    "for item in data:\n",
    "    print(item.text)\n",
    "\n",
    "driver.quit()\n",
    "```\n",
    "\n",
    "### 5. **APIs**\n",
    "\n",
    "- **Description**: Using the provided APIs (Application Programming Interfaces) of websites to access data in a structured format (JSON, XML).\n",
    "- **Use Case**: Preferred when a website offers an official API for data access.\n",
    "- **Pros**: Reliable, well-documented, and efficient.\n",
    "- **Cons**: Limited to the data and functionality provided by the API.\n",
    "\n",
    "**Example Using Requests Library to Access API**:\n",
    "```python\n",
    "import requests\n",
    "\n",
    "url = 'https://api.example.com/data'\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "\n",
    "for item in data:\n",
    "    print(item['field'])\n",
    "```\n",
    "\n",
    "### 6. **Web Scraping Services**\n",
    "\n",
    "- **Description**: Using third-party web scraping services like Scrapy, Octoparse, or ParseHub that offer user-friendly interfaces and powerful scraping capabilities.\n",
    "- **Use Case**: Suitable for non-programmers or complex scraping tasks.\n",
    "- **Pros**: Easy to set up, powerful, and can handle large-scale scraping.\n",
    "- **Cons**: May involve costs and depend on the service provider.\n",
    "\n",
    "### Summary\n",
    "\n",
    "1. **Manual Scraping**: Simple but time-consuming and not scalable.\n",
    "2. **Regular Expressions**: Fast for simple tasks but hard to maintain.\n",
    "3. **HTML Parsing Libraries**: Easy and powerful for structured HTML.\n",
    "4. **Browser Automation Tools**: Ideal for dynamic content but resource-intensive.\n",
    "5. **APIs**: Reliable and efficient for structured data access.\n",
    "6. **Web Scraping Services**: User-friendly and powerful but may involve costs.\n",
    "\n",
    "Selecting the appropriate method depends on the complexity of the website, the nature of the data, and the specific requirements of the scraping task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d2d693-da13-4cd5-9e9a-a8ad58989cd7",
   "metadata": {},
   "source": [
    "### Q3. What is Beautiful Soup? Why is it used?\n",
    "### Ans:-\n",
    "#### ### What is Beautiful Soup?\n",
    "\n",
    "**Beautiful Soup** is a Python library used for parsing HTML and XML documents. It creates a parse tree from page source code that can be used to extract data easily. The library is named after a poem in Lewis Carroll's \"Alice's Adventures in Wonderland.\"\n",
    "\n",
    "### Why is Beautiful Soup Used?\n",
    "\n",
    "Beautiful Soup is widely used for web scraping because it simplifies the process of navigating, searching, and modifying the parse tree. Here are some key reasons why it is used:\n",
    "\n",
    "1. **Ease of Use**:\n",
    "   - Beautiful Soup provides Pythonic idioms for iterating, searching, and modifying the parse tree, making it accessible for beginners and efficient for experienced programmers.\n",
    "\n",
    "2. **Handles Inconsistent HTML**:\n",
    "   - Beautiful Soup can parse broken HTML and XML documents, which is common in web scraping scenarios. It can automatically transform a complex HTML document into a structured format.\n",
    "\n",
    "3. **Integration with Other Libraries**:\n",
    "   - It can be easily integrated with other libraries like `requests` for fetching web content and `pandas` for data analysis and manipulation.\n",
    "\n",
    "4. **Powerful Searching Capabilities**:\n",
    "   - Beautiful Soup provides various methods to search and navigate through the parse tree, such as `find()`, `find_all()`, and CSS selectors.\n",
    "\n",
    "### Example Usage of Beautiful Soup\n",
    "\n",
    "Below is an example of using Beautiful Soup to scrape data from a website:\n",
    "\n",
    "1. **Install Beautiful Soup and Requests**:\n",
    "   First, ensure you have Beautiful Soup and requests installed. You can install them using pip:\n",
    "\n",
    "   ```bash\n",
    "   pip install beautifulsoup4 requests\n",
    "   ```\n",
    "\n",
    "2. **Write the Scraping Script**:\n",
    "\n",
    "   ```python\n",
    "   import requests\n",
    "   from bs4 import BeautifulSoup\n",
    "\n",
    "   # URL of the website to scrape\n",
    "   url = 'https://example.com'\n",
    "\n",
    "   # Send a GET request to the website\n",
    "   response = requests.get(url)\n",
    "\n",
    "   # Parse the HTML content of the webpage\n",
    "   soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "   # Find and print all headline elements (assuming they are in <h2> tags)\n",
    "   headlines = soup.find_all('h2', class_='headline')\n",
    "\n",
    "   for headline in headlines:\n",
    "       print(headline.text)\n",
    "   ```\n",
    "\n",
    "### Key Functions and Methods in Beautiful Soup\n",
    "\n",
    "1. **`BeautifulSoup(html, 'html.parser')`**:\n",
    "   - Initializes a Beautiful Soup object and parses the HTML content using the built-in HTML parser.\n",
    "\n",
    "2. **`soup.find(tag, attributes)`**:\n",
    "   - Finds the first occurrence of a tag with the specified attributes.\n",
    "\n",
    "3. **`soup.find_all(tag, attributes)`**:\n",
    "   - Finds all occurrences of a tag with the specified attributes.\n",
    "\n",
    "4. **`soup.select(selector)`**:\n",
    "   - Finds all elements matching a CSS selector.\n",
    "\n",
    "5. **Navigating the Parse Tree**:\n",
    "   - **`tag.name`**: Gets the tag name.\n",
    "   - **`tag['attribute']`**: Gets the value of an attribute.\n",
    "   - **`tag.text`**: Gets the text content of the tag.\n",
    "   - **`tag.contents`**: Gets the children of the tag as a list.\n",
    "\n",
    "### Summary\n",
    "\n",
    "**Beautiful Soup** is a powerful and flexible library for parsing HTML and XML in Python, making it ideal for web scraping tasks. It simplifies the process of extracting data from web pages by handling inconsistencies in HTML and providing easy-to-use methods for navigating and searching the parse tree. By integrating with other libraries, it allows for efficient data retrieval and analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ce1e81-d57d-4090-a95c-296f55d5e3bf",
   "metadata": {},
   "source": [
    "### Q4. Why is flask used in this Web Scraping project?\n",
    "### Ans:-\n",
    "#### Flask is often used in web scraping projects to create a web interface or an API for the scraping functionality. Here are several reasons why Flask is useful in such projects:\n",
    "\n",
    "#### 1. **Lightweight and Simple**\n",
    "\n",
    "- **Description**: Flask is a micro web framework, meaning it provides the essential tools to get a web server up and running without the overhead of more complex frameworks like Django.\n",
    "- **Use Case**: Ideal for small to medium-sized projects where simplicity and speed of development are important.\n",
    "\n",
    "#### 2. **Easy to Set Up**\n",
    "\n",
    "- **Description**: Flask is easy to install and set up. With just a few lines of code, you can have a web server running.\n",
    "- **Use Case**: Allows quick development and testing of the web scraping interface.\n",
    "\n",
    "#### 3. **Flexible and Extensible**\n",
    "\n",
    "- **Description**: Flask is highly flexible and allows you to add only the components you need. You can easily integrate various extensions for added functionality (e.g., Flask-RESTful for API development, Flask-SQLAlchemy for database interactions).\n",
    "- **Use Case**: Customizable to meet the specific needs of your web scraping project.\n",
    "\n",
    "#### 4. **RESTful API Creation**\n",
    "\n",
    "- **Description**: Flask makes it straightforward to create RESTful APIs, which can be used to interact with the web scraping functions.\n",
    "- **Use Case**: Enables you to build endpoints that trigger the scraping process, retrieve data, or perform other related tasks.\n",
    "\n",
    "#### 5. **Template Rendering**\n",
    "\n",
    "- **Description**: Flask comes with Jinja2, a powerful templating engine that allows you to dynamically generate HTML pages.\n",
    "- **Use Case**: Useful for creating a front-end interface where users can input parameters for scraping or view the scraped data.\n",
    "\n",
    "#### 6. **Good Documentation and Community Support**\n",
    "\n",
    "- **Description**: Flask has excellent documentation and a large, active community. This means that finding solutions to problems or learning how to implement new features is relatively easy.\n",
    "- **Use Case**: Provides support and resources for developers to troubleshoot and extend their applications.\n",
    "\n",
    "#### Example: Using Flask in a Web Scraping Project\n",
    "\n",
    "Here’s a simple example demonstrating how Flask can be used in a web scraping project to create a web interface that triggers a scraping function and displays the results.\n",
    "\n",
    "1. **Install Flask and Required Libraries**:\n",
    "   ```bash\n",
    "   pip install flask beautifulsoup4 requests\n",
    "   ```\n",
    "\n",
    "2. **Create the Flask Application**:\n",
    "\n",
    "   ```python\n",
    "   from flask import Flask, render_template, request, jsonify\n",
    "   import requests\n",
    "   from bs4 import BeautifulSoup\n",
    "\n",
    "   app = Flask(__name__)\n",
    "\n",
    "   @app.route('/')\n",
    "   def home():\n",
    "       return render_template('index.html')\n",
    "\n",
    "   @app.route('/scrape', methods=['POST'])\n",
    "   def scrape():\n",
    "       url = request.form['url']\n",
    "       response = requests.get(url)\n",
    "       soup = BeautifulSoup(response.content, 'html.parser')\n",
    "       headlines = [h2.text for h2 in soup.find_all('h2')]\n",
    "       return jsonify(headlines=headlines)\n",
    "\n",
    "   if __name__ == '__main__':\n",
    "       app.run(debug=True)\n",
    "   ```\n",
    "\n",
    "3. **Create the Template (`templates/index.html`)**:\n",
    "\n",
    "   ```html\n",
    "   <!doctype html>\n",
    "   <html lang=\"en\">\n",
    "     <head>\n",
    "       <meta charset=\"UTF-8\">\n",
    "       <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "       <title>Web Scraping with Flask</title>\n",
    "     </head>\n",
    "     <body>\n",
    "       <h1>Enter URL to Scrape</h1>\n",
    "       <form action=\"/scrape\" method=\"post\">\n",
    "         <input type=\"text\" name=\"url\" placeholder=\"Enter URL\" required>\n",
    "         <button type=\"submit\">Scrape</button>\n",
    "       </form>\n",
    "       <h2>Headlines:</h2>\n",
    "       <ul id=\"headlines\"></ul>\n",
    "\n",
    "       <script>\n",
    "         document.querySelector('form').addEventListener('submit', async function (event) {\n",
    "           event.preventDefault();\n",
    "           const form = event.target;\n",
    "           const formData = new FormData(form);\n",
    "           const response = await fetch(form.action, {\n",
    "             method: form.method,\n",
    "             body: formData\n",
    "           });\n",
    "           const data = await response.json();\n",
    "           const headlinesList = document.getElementById('headlines');\n",
    "           headlinesList.innerHTML = '';\n",
    "           data.headlines.forEach(headline => {\n",
    "             const li = document.createElement('li');\n",
    "             li.textContent = headline;\n",
    "             headlinesList.appendChild(li);\n",
    "           });\n",
    "         });\n",
    "       </script>\n",
    "     </body>\n",
    "   </html>\n",
    "   ```\n",
    "\n",
    "#### Summary\n",
    "\n",
    "Using Flask in a web scraping project provides several benefits:\n",
    "- **Simplicity**: Easy to set up and get started.\n",
    "- **Flexibility**: Can be tailored to specific needs with various extensions.\n",
    "- **API Development**: Simplifies the creation of RESTful APIs to interact with the scraping functions.\n",
    "- **Template Rendering**: Enables the creation of dynamic web pages for user interaction.\n",
    "- **Community Support**: Access to extensive documentation and community resources. \n",
    "\n",
    "This combination of features makes Flask a popular choice for developing web interfaces and APIs in web scraping projects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9216609-abfc-465e-a577-49bce73e54d2",
   "metadata": {},
   "source": [
    "### Q5. Write the names of AWS services used in this project. Also, explain the use of each service.\n",
    "### Ans:-\n",
    "#### AWS offers a wide range of services that can be utilized in a web scraping project to enhance its functionality, scalability, and reliability. Below are some commonly used AWS services in such a project, along with their uses:\n",
    "\n",
    "### 1. **Amazon EC2 (Elastic Compute Cloud)**\n",
    "- **Use**: Provides scalable virtual servers to run web scraping scripts and Flask applications.\n",
    "- **Details**: \n",
    "  - You can launch instances with varying compute power to handle the processing requirements of your scraping tasks.\n",
    "  - Enables you to scale up or down based on the workload.\n",
    "\n",
    "### 2. **Amazon S3 (Simple Storage Service)**\n",
    "- **Use**: Storage for scraped data and other assets like logs, images, and backups.\n",
    "- **Details**:\n",
    "  - Provides highly durable and scalable object storage.\n",
    "  - Allows you to store large amounts of data cost-effectively.\n",
    "\n",
    "### 3. **Amazon RDS (Relational Database Service)**\n",
    "- **Use**: Manages relational databases for storing structured scraped data.\n",
    "- **Details**:\n",
    "  - Supports various database engines like MySQL, PostgreSQL, and MariaDB.\n",
    "  - Handles database management tasks such as backups, patching, and scaling.\n",
    "\n",
    "### 4. **Amazon DynamoDB**\n",
    "- **Use**: NoSQL database for storing and querying unstructured or semi-structured scraped data.\n",
    "- **Details**:\n",
    "  - Provides low-latency and high-performance database operations.\n",
    "  - Automatically scales to handle large amounts of data and traffic.\n",
    "\n",
    "### 5. **AWS Lambda**\n",
    "- **Use**: Runs code in response to events without provisioning or managing servers (serverless computing).\n",
    "- **Details**:\n",
    "  - Ideal for running short-lived web scraping tasks.\n",
    "  - Can be triggered by events such as S3 uploads or DynamoDB updates.\n",
    "\n",
    "### 6. **Amazon CloudWatch**\n",
    "- **Use**: Monitoring and logging service to track the performance and health of your scraping infrastructure.\n",
    "- **Details**:\n",
    "  - Collects and visualizes metrics and logs from EC2 instances, Lambda functions, and other AWS services.\n",
    "  - Helps in setting up alarms to notify you of any issues.\n",
    "\n",
    "### 7. **Amazon SQS (Simple Queue Service)**\n",
    "- **Use**: Decouples and scales microservices, distributed systems, and serverless applications by using message queues.\n",
    "- **Details**:\n",
    "  - Ensures reliable communication between components of the scraping system.\n",
    "  - Handles message queuing for tasks such as sending URLs to be scraped to various workers.\n",
    "\n",
    "### 8. **AWS IAM (Identity and Access Management)**\n",
    "- **Use**: Manages access to AWS services and resources securely.\n",
    "- **Details**:\n",
    "  - Provides fine-grained access control.\n",
    "  - Ensures that only authorized users and services can perform specific actions on the AWS resources.\n",
    "\n",
    "### 9. **AWS CloudFormation**\n",
    "- **Use**: Automates the setup and management of AWS resources.\n",
    "- **Details**:\n",
    "  - Allows you to define your infrastructure as code.\n",
    "  - Makes it easy to replicate the infrastructure setup across multiple environments.\n",
    "\n",
    "### 10. **Amazon API Gateway**\n",
    "- **Use**: Creates, publishes, maintains, monitors, and secures APIs at any scale.\n",
    "- **Details**:\n",
    "  - Facilitates the creation of RESTful APIs to interact with your scraping and data processing functions.\n",
    "  - Provides features like throttling, caching, and authorization.\n",
    "\n",
    "### Example Use Case in a Web Scraping Project\n",
    "\n",
    "1. **Compute**:\n",
    "   - Use **Amazon EC2** to run the web scraping scripts and Flask application.\n",
    "   - Use **AWS Lambda** for serverless execution of scraping tasks triggered by specific events.\n",
    "\n",
    "2. **Storage**:\n",
    "   - Store the scraped data in **Amazon S3** for later processing or analysis.\n",
    "   - Use **Amazon RDS** for structured data storage or **Amazon DynamoDB** for unstructured data storage.\n",
    "\n",
    "3. **Database**:\n",
    "   - Use **Amazon RDS** to store structured scraped data (e.g., MySQL, PostgreSQL).\n",
    "   - Use **Amazon DynamoDB** for unstructured or semi-structured data.\n",
    "\n",
    "4. **Monitoring and Logging**:\n",
    "   - Use **Amazon CloudWatch** to monitor the performance and health of your scraping infrastructure and log the scraping activities.\n",
    "\n",
    "5. **Queue Management**:\n",
    "   - Use **Amazon SQS** to manage and queue scraping tasks, ensuring that they are processed reliably and efficiently.\n",
    "\n",
    "6. **Security and Access Management**:\n",
    "   - Use **AWS IAM** to control access to your AWS resources and services.\n",
    "\n",
    "7. **Infrastructure as Code**:\n",
    "   - Use **AWS CloudFormation** to define and manage the infrastructure setup for your scraping project.\n",
    "\n",
    "8. **API Management**:\n",
    "   - Use **Amazon API Gateway** to create APIs that expose your scraping and data retrieval functionalities to external clients or applications.\n",
    "\n",
    "### Summary\n",
    "\n",
    "Using these AWS services together provides a robust, scalable, and flexible infrastructure for web scraping projects. They help manage various aspects of the project, from compute resources and data storage to monitoring, security, and API management, ensuring that the scraping tasks are performed efficiently and reliably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2276ac-819c-44da-bdbd-dd0f7bb20fc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e33a678-343f-4946-9093-3cd6706b3df9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128ae77f-05d6-4c51-81da-46721f29962a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4564eb8-00f6-4077-a659-ac7cfc9cc7df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b985e130-bbb4-4257-aaec-7e602085a1a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b514b37-3fd4-4aa1-83ad-684026504ca7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5d7b4b-a77b-4c4e-9edf-e6c6337d2c8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbef33b-b65d-4233-8b5f-7c0b007028e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96600a57-f636-484a-a5fc-6dd0b018476c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1ee72f-a9c3-4fb9-b1b1-cd8d23f9da4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565445a6-0e13-4293-beda-b235e75d07d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ed11b0-359e-4ff5-a361-eb67d020d02f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2358c0-9dab-4f84-93cc-6c46541e8779",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cb8c39-3491-4863-8968-1419f44d8dd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507be115-b359-4cde-813f-2f6af85695ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5c747f-c564-4beb-965f-da7f8fb55cb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9761e5a-4220-4daa-8ede-f50ba8113b05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d27c80-245e-4d8f-9962-ba90cfebdec4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e505100b-5533-4b3a-ab59-e8130b76a440",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812fe185-c7c0-456f-a6e6-96a57200bc4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530dcf56-e63d-4673-bb7c-fc4cd10dfd8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
